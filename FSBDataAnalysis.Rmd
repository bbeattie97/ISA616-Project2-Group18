---
title: "FSB Data Analysis Group 18"
author: "Jake Holroyd, Brendan Beattie"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_download: yes
  word_document:
    toc: no
---

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# FSB Data Analysis: Group 18 {.unnumbered}

# Research Question {.unnumbered}

**"What are the macro placement and salay trends FSB is seeing over the past three years"**

-   Are our placement results going up / down / steady?
-   Salary trends?
-   Kirk would like to see this for FSB overall as well as by major

# Business Value Proposition {.unnumbered}

We have created a business value proposition for the FSB Career Services that reports to Kirk Bogard, the Associate Vice President for Development and External Relations in FSB.They are interested in how the FSB Career Services advising team can more effectively direct their efforts to properly serve the almost 5,000 students in FSB.


## Visualization {.unnumbered}

![](Data/images/FSBDataBVP.png)

## Text {.unnumbered}

**1. Define the client or audience for your data analysis solution.**

- The FSB Career Services team that reports to Kirk Bogard, Associate Vice President for Development and External Relations in FSB.

- We are tasked with assisting with their duties by providing insight and recommendations on how Careers Services can best advise FSB undergraduate students.

- In context, we have been asked to explore the macro placement and salary trends FSB has seen over the past three years.



**2. Define the client jobs. What jobs are your clients trying to get done? Use a separate bullet point for each job you intend to help your customer get done. Be specific.**

- Adjusting advising efforts to reflect placement and salary trends.

- Effectively aid students in their pursuit of careers.

- Understand how students of various majors differ in job placement and expected salary.

- Develop solutions that equip FSB students to place well and earn high.


**3. Add pains. What are the pains before, during, and after your customer is trying to get their jobs done? Create a bullet point for every specific pain.**

- There are about 5,000 students in FSB and the staff tasked with advising them is significantly smaller (4) so it is impossible to adequately and thoroughly advise all 5,000 students.

- Some majors may have an easier time placing in jobs or earning higher salaries. Career Services would like to know which ones, or else they can’t focus on those in the most need. 

- Career Services can’t prepare students who may have a more difficult time in their job search or inform students with accurate salary expectations if they don’t know which majors are experiencing hiring surges or hiring decreases based on recent trends. 


**4. Describe your intended solution. Give a bullet point for each major element of your product or solution.**

- Compare the data of both job placement trends and salary trends over the three year time period provided.

- Deliver an analysis to show job placement and salary trends over time across the entirety of FSB, as well as drilling down to explore results by major.

- Create visualizations that illustrate these trends in an understandable and digestible way to show changes in trends by major and be able to easily compare the most recent differences in the department.


**5. What are the painkillers from your solution? List out those things in your solution that can kill the pain points of your client.**

- Creating understandable and easily comparable visualizations allows the small team of career advisors to more effectively and efficiently direct their efforts across the vast student body so they can focus on those in the most need.

- Deliver a sound understanding of placement result trends by major through the creation of visualizations that compare changes and reveal trends over the three years, informing Career Services of which majors are most in need of advising. 

- Be able to provide Career Services with relevant job market information based on trends identified in the past three years to allow them to help students understand personal market value based on their major and education at FSB.

**6. Add gains. What are the gains that your customer expects to get or will be surprised to benefit from your solution?**

- The Career Services team will be able to equip students with the necessary insights and information to prepare them for their job search.

- Career Services will be able to effectively target advising efforts toward students in the most need.

- Career Services will be able to provide students with expectations that will be more accurate to reality regarding career search, ideally resulting in less reliance on the advising team, allowing them time to help even more students.


**7. What are the gain creators? List out the items that will add additional value to your customer, above and beyond addressing their current pain points.**

- The analysis over time and subsequent visualizations will be easily reproducible. Career Services will be able to routinely update them year after year with fresh data to track the placement and salary trends well into the future as needed.

- The analysis over time and visualizations will allow Career Services to track their own progress in their efforts, providing feedback if used over time on how their newly directed efforts have impacted placement and salary trends.

# The Data {.unnumbered}

## Data Sources {.unnumbered}

We have been given three years of data representing FSB graduates, including graduates in 2019, 2020, and 2021. The data set provided had 42 variables.  The source is either derived by Dr. Jones-Farmer during data cleaning/merging, from the Oracle Business Intelligence Enterprise Edition (OBIEE) maintained by Miami administration, or from the self reported senior survey.  Dr. Jones-Farmer has cleaned and merged the files into one file.  

1.  major: numeric,derived, the number of majors 
2.  major1: text, OBIEE, first major
3.  major 2: text, OBIEE, second major
4.  BBRJ: binary, OBIEE, an attribute of a student, but we do not know what this stands for
5.  Business Direct Admit: binary, OBIEE, a direct admit to FSB as a first year
6.  Combined Cacc and Masters: binary, OBIEE, combined degree student
7.  Dean's List: binary, OBIEE, achieve dean's list status at least once
8.  First Generation College Stdnt: binary, OBIEE, first generation student status
9.   FSB Scholars: binary, OBIEE, FSB scholars program
10.  Honors Program: binary, OBIEE, member of University honors program
11.  President's list: binary, OBIEE, achieved president's list at least once
12.  Study Abroud Courtesy Account: binary, OBIEE, do not know meaning
13.  Transfer Work: binary, OBIEE, do not know exact meaning
14.  Cum Laude: binary, OBIEE, graduated Cum Laude
15.  Magna Cum Laude: binary, OBIEE, graduated Magna Cum Laude
16.  Summa Cum Laude: binary, OBIEE, graduated Summa Cum Laude
17.  University Honors: binary, OBIEE, graduated with University Honors
18.  University Honors w/Distinction: binary, OBIEE, graduated with University Honors with Distinction
19.  minor1: text, OBIEE, first listed minor
20.  minor2: text, OBIEE, second listed minor
21.  IPEDS.Race.Ethnicity: text, OBIEE, race/ethnicity
22.  Gender: text, OBIEE, sex
23.  GPA.Range: text, OBIEE, GPA within a .5 range
24.  Term.Code: numberic, OBIEE, First four digits are the physcal year (beginning in July, e.g. July 2020 is FY 2021).  Last two digits is the term (10=fall, 15=winter, 20=spring, 30=summer).
25.  Year.x: text, derived, first four digits of Term.Code stored as a character variable
26.  latin_honors: text, survey, latin honors designation
27.  survey_city: text, survey, student reported city in which their job is located
28.  survey_company: text, survey, student reported company in which they accepted a job
29.  survey_deptfunc: text, survey, student reported job function
30.  survey_gradprogram: text, survey, student reported graduate program they will be attending
31.  survey_gradschool: text, survey, stuent reported graduate school they will be attending
32.  survey_internfour: text, survey, student reported fourth internship they held during college
33.  survey_internthree: text, survey, student reported third internship they held during college
34.  survey_interntwo: text, survey, student reported second internship they held during college
35.  survey_internone: text, survey, student reported first internship they held during college
36.  Survey_internships: text, survey, Student reported number of internships they held during college
37.  survey_offers: text, survey, student reported number of offers for full time employment received
38.  survey_plans: text, survey, student reported plans after graduation
39.  survey_pref_field: text, survey, student reported whether working in preferred field
40.  survey_pref_loc: text, survey, student reported whether working in preferred location
41.  survey_salary: numeric, survey, student reported salary
42.  survey_state: text, survey, student reported state in which job is located


## Data Cleaning {.unnumbered}

```{r}
## LOADING DATA##
FSB  = readRDS("FSB_BI_Survey_2019_2021.rds")

head(FSB)
```

For the purposes of our analysis, we are looking at the job placement and salary trends for FSB graduates as a whole and by major. For this, we should only be looking at students who are intending to enter the workforce and exclude those who's plans deviate from that course (i.e. continuing their education). 

```{r}
unique(FSB$survey_plans)
```

### Exploring Context {.unnumbered}
We want to look at various response to evaluate what context they provide to our analysis and whether they would be useful to keep in the final data set we use.

Primarily, we want to look at responses in the survey_plans field because we believe that will be a strong way to filter out observations that have no correlation to the job placement and salary trends. 

For example: "seeking continuing education", "continuing education", "not seeking employment", NA are assumed not relevant to our purposes. If an individual is not actively trying to get a job or accepting a job, then the data they provide will not be useful in determining job placement and salary trends.

Contrarily: "seeking employment", "accepted parttime job" and "accepted fulltime job" are assumed relevant to our purposes and will be included in the analysis. The data they provide will grant us insight on job placement and salary trends because they are actively attempting to secure jobs. We will need to impute missing values and clean any non-standardized responses (i.e. the company or city fields that have various capitilazation and spelling differences)
```{r}
## Exploring responses to evaluate context ##

FSB_self = FSB |> dplyr::filter(FSB$survey_plans == "self employed") 
# Does not appear relevant for our purposes. Only one response with this designation and it does not include values in the fields necessary for our analysis.

FSB_other = FSB |> dplyr::filter(FSB$survey_plans == "other") 
# Does not appear relevant for our purposes. No values for preferred field, location, salary, etc.

FSB_mil = FSB |> dplyr::filter(FSB$survey_plans == "military service")
# Does not appear relevant for our purposes. No values for preferred field, location, salary, and though they technically work for the military they have very limited responses in the company field

# remove unnecessary dfs
remove(FSB_other)
remove(FSB_self)
remove(FSB_mil)
```

Now that we have our list of response values we would like to filter on, we will now create our new data set

```{r}
# create a new data set that filters out irrelevant observations
FSB_job = FSB |> dplyr::filter(FSB$survey_plans %in%
                                 c("seeking employment", "accepted parttime job",
                                   "accepted fulltime job"))
```




### Counting Missing by Variable {.unnumbered}
```{r}
sapply(FSB_job, function(x) sum(is.na(x)))
```
It seems clear that the survey responses contain a substantial amount of observations with missing values across the entirety of the survey fields. It is our conclusion that to omit responses with missing values would destroy any chance of pulling any meaningful analysis from the data. With that assumption, we will instead clean and impute missing values for fields necessary for our analysis.


### Cleaning Non-standard responses {.unnumbered}

Before we make standardize our city and company variables, we need to clean them. First we made all of the responses all capitalized. This will take out the need to correct capitalization issues when we standardize the data. Next we will sort the responses by frequency and collect the top 20. These lists of 20 will be the variables we will focus in our data set.
```{r}
FSB_job$survey_company = toupper(FSB_job$survey_company)

company = FSB_job$survey_company
top20company = as.data.frame(table(company))
companysorted = top20company[order(-top20company$Freq), ]
top20_company_names = companysorted$company[1:20]

# View the vector or list of top 20 company names
print(top20_company_names)
```

```{r}
FSB_job$survey_city = toupper(FSB_job$survey_city)

city = FSB_job$survey_city
top20city = as.data.frame(table(city))
citysorted = top20city[order(-top20city$Freq), ]
top20_city_names = citysorted$city[1:20]

# View the vector or list of top 20 company names
print(top20_city_names)
```


#### Standardization (Companies) {.unnumbered}
One of the major roadblocks we identified with the response data is how often students entered different spellings of the same words. To solve this issue we need to standardize our data to ensure that all of the data is standardized. We decided to focus on the 20 most popular cities and companies to standardize. First we want to see what the top 20 companies are so we know which companies to standardize.

```{r}
print(top20_company_names)
```

Now that we know which companies we are going to focus on, it is time to standardize our data. To do this we will be printing all of the companies from out data set that start with the letter of the company in question. Next we will observe all of the various entries and change then all to have the same spelling.

```{r}
##EY MAPPING
jobs_e = FSB_job[grep('^E', FSB_job$survey_company),]
table(jobs_e$survey_company)
EY_map = c("ERNST & YOUNG (EY)" = "EY", 
           "ERNST AND YOUNG" = "EY",
           "ERNST & YOUNG" = "EY")
```

```{r}
##DELOITTE MAPPING
jobs_d = FSB_job[grep('^D', FSB_job$survey_company),]
table(jobs_d$survey_company)
DELOITTE_mapping = c("DELOITTE CONSULTING" = "DELOITTE",
                 "DELOITTE LLP"= "DELOITTE",
                 "DELOITTE TAX"= "DELOITTE",
                 "DELOITTE, CHINA" = "DELOITTE")

## DHL SUPPLY CHAIN MAPPING
DHL_SC_mapping = c("DHL SUPPLY CHAIN" = "DHL")
```

```{r}
##PWC MAPPING
jobs_p = FSB_job[grep('^P', FSB_job$survey_company),]
table(jobs_p$survey_company)
PWC_mapping = c( "PWC CORPORATE FINANCE"= "PWC",
             "PRICEWATERHOUSE COOPERS" = "PWC",
             "PWC CORPORATE FINANCE LLC" = "PWC", 
             "PRICEWATERHOUSECOOPERS" = "PWC")

## PEPSICO MAPPING
PepsiCo_mapping = c("PEPSICO-FRITO LAY" = "PEPSICO",
                    "PEPSICO FRITO-LAY" = "PEPSICO",
                    "FRITO LAY"  = "PEPSICO",
                    "FRITO-LAY" = "PEPSICO")
## PROTIVITI MAPPING NOT NEEDED
```

```{r}
##TEXTRON MAPPING
jobs_t = FSB_job[grep('^T', FSB_job$survey_company),]
table(jobs_t$survey_company)
Textron_mapping = c("TEXTRON AVIATION" = "TEXTRON",
                "TEXTRON SYSTEMS: LYCOMING ENGINES"= "TEXTRON",
                 "TEXTRON SPECIALIZED VEHICLES" = "TEXTRON",
                "TEXTRON SYSTEMS" = "TEXTRON"
                )
##TERILLIUM MAPPING
Terillium_mapping = c("TERRILIUM" = "TERILLIUM")
```

```{r}
##ABERCROMBIE & FITCH MAPPING
jobs_a = FSB_job[grep('^A', FSB_job$survey_company),]
table(jobs_a$survey_company)
A_and_F_mapping = c("ABERCROMBIE & FITCH" = "ABERCROMBIE")
##ARRIVE LOGISTICS MAPPING NOT NEEDED
```

```{r}
jobs_j = FSB_job[grep('^J', FSB_job$survey_company),]
table(jobs_j$survey_company)
JP_Morgan_Chase_mapping = c("J.P. MORGAN U.S. PRIVATE BANK" = "JP MORGAN",
                       "JPMORGAN CHASE" = "JP MORGAN",
                       "J.P MORGAN" = "JP MORGAN", 
                       "JP MORGAN & CO." = "JP MORGAN",
                       "J.P. MORGAN" = "JP MORGAN",
                       "JP MORGAN AND CHASE" = "JP MORGAN",
                       "JP MORGAN CHASE" = "JP MORGAN")

```

```{r}
## COYOTE LOGISTICS MAPPING
jobs_c = FSB_job[grep('^C', FSB_job$survey_company),]
table(jobs_c$survey_company)
coyote_mapping = c("COYOTE LOGISTICS" = "COYOTE")
## CDW MAPPING NOT NEEDED

##CROWE LLP MAPPING
Crowe_mapping = c("CROWE LLP" = "CROWE")
```

```{r}
## FIFTH THIRD BANK MAPPING
jobs_f = FSB_job[grep('^F', FSB_job$survey_company),]
table(jobs_f$survey_company)
Fifth_Third_Bank_mapping = c("FIFTH-THIRD BANK" = "FIFTH THIRD BANK")
```

Now that we have created mapping variables for all of the companies that needed standardizing, it is time to commit the desired changes to our data set. We will do this by replacing all of the other spellings to our desired choice that we created in our mapping. 

```{r}
## Updating Data set with standardized companies
library(tidyverse)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, A_and_F_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, coyote_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, Crowe_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, DELOITTE_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, DHL_SC_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, EY_map)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, Fifth_Third_Bank_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, JP_Morgan_Chase_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, PepsiCo_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, PWC_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, Terillium_mapping)
FSB_job$survey_company = str_replace_all(FSB_job$survey_company, Textron_mapping)
```

Next, we want to change all of the companies that are outside of our top 20 to "other". 
```{r}

top_companies = c("DELOITTE", "EY", "TEXTRON", "PWC", "JP MORGAN", "KPMG", "ABERCROMBIE", "PEPSICO", "DHL", "RSM", "COYOTE", "ARRIVE LOGISTICS", "CDW", "FIFTH THIRD BANK", "TERILLIUM", "WEST MONROE PARTNERS", "PROTIVITI", "CROWE", "INSIGHT2PROFIT", "KEYBANK")

FSB_job$survey_company <-  ifelse(!(toupper(FSB_job$survey_company) %in% toupper(top_companies)), "OTHER", toupper(FSB_job$survey_company ))
table(FSB_job$survey_company)
```

#### Standardization (Cities) {.unnumbered}

We will repeat the same standardization process with cities collected from our survey. First we idenify the top 20 cities to focus on. 
```{r}
print(top20_city_names)
```

```{r}
print(top20_city_names)
```

We then create mapping for our standardization.
```{r}
## CINCINNATI MAPPING
city_c = FSB_job[grep('^C', FSB_job$survey_city),]
table(city_c$survey_city)

Cincinnati_mapping = c("CINCINATTI" = "CINCINNATI",
                       "CINCINNATI, OH" = "CINCINNATI",
                       "CINCINNNATI" = "CINCINNATI")
## COLUMBUS MAPPING
Columbus_mapping = c("COLUBMUS" = "COLUMBUS")
```
```{r}
## NEW YORK MAPPING
city_n = FSB_job[grep('^N', FSB_job$survey_city),]
table(city_n$survey_city)

New_York_mapping = c("NEW YORK CITY" = "NEW YORK",
                     "NYC" = "NEW YORK")
```
```{r}
## INDIANAPOLIS MAPPING
city_i = FSB_job[grep('^I', FSB_job$survey_city),]
table(city_i$survey_city)

Indianapolis_mapping = c("INDIANPOLIS" = "INDIANAPOLIS")
```
```{r}
## MILLWAUKEE MAPPING
city_m = FSB_job[grep('^M', FSB_job$survey_city),]
table(city_m$survey_city)

Milwaukee_mapping = c("MILWAUKWEE" = "MILWAUKEE")
```

Now we update our main data set with our newly created standardized mapping.
```{r}
## Updating Data set with standardized cities
FSB_job$survey_city = str_replace_all(FSB_job$survey_city, Cincinnati_mapping)
FSB_job$survey_city = str_replace_all(FSB_job$survey_city, Columbus_mapping)
FSB_job$survey_city = str_replace_all(FSB_job$survey_city, Indianapolis_mapping)
FSB_job$survey_city = str_replace_all(FSB_job$survey_city, Milwaukee_mapping)
FSB_job$survey_city = str_replace_all(FSB_job$survey_city, New_York_mapping)

```

Since we are only looking at the top 20 cities, we decided to change all of the cities outside of the top 20 to other. 
```{r}
top_cities = c("CHICAGO", "CINCINNATI", "COLUMBUS", "CLEVELAND", "NEW YORK", "INDIANAPOLIS", "DENVER", "BOSTON", "PITTSBURGH", "DAYTON", "CHARLOTTE", "DETROIT", "MINNEAPOLIS", "SAN FRANCISCO", "DUBLIN", "MILWAUKEE", "AKRON", "TOLEDO", "ATLANTA", "LOS ANGELES")

FSB_job$survey_city <-  ifelse(!(toupper(FSB_job$survey_city) %in% toupper(top_cities)), "OTHER", toupper(FSB_job$survey_city ))
table(FSB_job$survey_city)
```

### Imputation {.unnumbered}


#### Imputing Categorical {.unnumbered}

For the remainder of relevant variables with missing values, we will impute the value of "Missing" to fill any null values.

```{r}
FSB_job = FSB_job |> 
  dplyr::mutate(
    survey_city = ifelse(is.na(survey_city), "Missing", survey_city),
    survey_company = ifelse(is.na(survey_company), "Missing", survey_company),
    survey_pref_field = ifelse(is.na(survey_pref_field), "Missing", survey_pref_field),
    survey_pref_loc = ifelse(is.na(survey_pref_loc), "Missing", survey_pref_loc),
    survey_state = ifelse(is.na(survey_state), "Missing", survey_state)
    )

# check missing
sapply(FSB_job, function(x) sum(is.na(x)))
```


#### Mean Salary by Major {.unnumbered}

After examining the missing value counts, we noticed that survey_salary had a significant portion of missing values. With this being such an important part of our analysis, we felt it would be inappropriate to remove the observations with missing salary data. We know that common practice is to impute the mean of numerical variables and so we will impute salary based on the mean salary reported by major of the student. We chose to impute by major to account for the variation in salary influenced by major (some majors may have a higher average salary).


```{r}
pacman::p_load("dplyr")
FSB_job = FSB_job |> 
  group_by(major1) |> 
  dplyr::mutate( survey_salary = ifelse(is.na(survey_salary), mean(survey_salary, na.rm = TRUE), survey_salary)) |> 
  ungroup()
```

Now, since we included the reported plan value of "seeking employment, any observation with that plan value will have a null value for salary. Since these individuals do not have jobs, we will impute $0 as their salary.

```{r}
FSB_job = FSB_job |> 
  dplyr::mutate( survey_salary = ifelse(survey_plans == "seeking employment", 0, survey_salary))
```

We will check our missing count for salary once again

```{r}
sum(is.na(FSB_job$survey_salary))
```

There is still one missing value in this column. We will subset to find this observation

```{r}
print(FSB_job[which(is.na(FSB_job$survey_salary)), ])
```
```{r}
# view a collection of observations with this specific major1 value
MandO = FSB_job |> dplyr::filter(FSB_job$major1 == "Management and Organizations") 
```

It appears that the missing value that remains is due to the fact that only one observation with this given major1 value is recorded, and they failed to report their salary. This means that mean imputation by major will not be able to fill in their salary and therefore the data this observation provides is not relevant to our purposes. We will remove this observation.

```{r}
# remove unnecessary df
remove(MandO)

# drop the observation
FSB_job =  FSB_job[-(which(is.na(FSB_job$survey_salary))), ]

# check missing
sum(is.na(FSB_job$survey_salary))
```
Now our data is clean and ready for use in the analysis. The purpose of setting the data up the way we did is that now any visualizations we create will be easily understandable in context, and will have to most relevant data contributing to their creation. We attempted to limit ambiguity and interference 

```{r}
str(FSB_job$year.x)
```



```{r}
write.csv(FSB_job, 'FSB_job.csv', row.names = )
```



## Printing our session information {.unnumbered}
```{r}
sessionInfo()
```